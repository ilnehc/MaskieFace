{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FaceNet_recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3Xx5IPTMgr3e"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ayzu6jgdBz1h"
      },
      "source": [
        "# ignore warnings\n",
        "import warnings\n",
        "import logging\n",
        "logging.captureWarnings(True)\n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.warn(\"This is a DeprecationWarning\",category=DeprecationWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1iWKuMxNjC3"
      },
      "source": [
        "warnings.filterwarnings('ignore', message='out of')\n",
        "warnings.filterwarnings('ignore', message='triggered')\n",
        "warnings.filterwarnings('ignore', message='Tracing')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNG92vlZLNw6"
      },
      "source": [
        "pip install mtcnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6Rr0qWIj2MF"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZekXF_KWMrt"
      },
      "source": [
        "# Data Preprocessing Using Facenet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE4sEEXsQBZI"
      },
      "source": [
        "import logging\n",
        "import os\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # FATAL\n",
        "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
        "# logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPi8jQ62R8Ok"
      },
      "source": [
        "import tensorflow as tf\n",
        "# tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RB1DiJ0Uf-D"
      },
      "source": [
        "from os import listdir\n",
        "from os.path import isdir\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot\n",
        "from numpy import load\n",
        "from numpy import savez_compressed\n",
        "from numpy import asarray\n",
        "from mtcnn.mtcnn import MTCNN\n",
        "\n",
        "\n",
        "# extract a single face from a given photograph\n",
        "def extract_face(filename, required_size=(160, 160)):\n",
        "  image = Image.open(filename)\n",
        "  image = image.convert('RGB')\n",
        "  # print(filename)\n",
        "  pixels = asarray(image)\n",
        "  detector = MTCNN()\n",
        "  results = detector.detect_faces(pixels)\n",
        "  # print(results)\n",
        "  x1, y1, width, height = results[0]['box']\n",
        "  x1, y1 = abs(x1), abs(y1)\n",
        "  x2, y2 = x1 + width, y1 + height\n",
        "  face = pixels[y1:y2, x1:x2]\n",
        "  image = Image.fromarray(face)\n",
        "  image = image.resize(required_size)\n",
        "  face_array = asarray(image)\n",
        "  return face_array\n",
        " \n",
        "# load images and extract faces for all images in a directory\n",
        "def load_faces(directory, required_size=(160, 160)):\n",
        "  count = 0\n",
        "  faces = list()\n",
        "  for filename in listdir(directory):\n",
        "    #if count == 5:\n",
        "    #  break\n",
        "    path = directory + filename\n",
        "    try:\n",
        "      face = extract_face(path, required_size)\n",
        "    except:\n",
        "      continue\n",
        "    faces.append(face)\n",
        "    count += 1\n",
        "  return faces\n",
        " \n",
        "# load a dataset \n",
        "def load_dataset(directory, required_size=(160, 160)):\n",
        "  X, y = list(), list()\n",
        "  for subdir in listdir(directory):\n",
        "    path = directory + subdir + '/'\n",
        "    if not isdir(path):\n",
        "      continue\n",
        "    faces = load_faces(path, required_size)\n",
        "    labels = [subdir for _ in range(len(faces))]\t\n",
        "    print('>loaded %d examples for class: %s' % (len(faces), subdir))\n",
        "    X.extend(faces)\n",
        "    y.extend(labels)\n",
        "  return asarray(X), asarray(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gj7e5ZGZF-sS"
      },
      "source": [
        "from numpy import concatenate\n",
        "#trainX, trainy = load_dataset('/content/drive/Shared drives/504 proj/OUR_DATASET/lfw_mixed_dataset/train_with_mask_masked_cloth_high/')\n",
        "#print(trainX.shape, trainy.shape)\n",
        "#train_data = load('/content/drive/Shared drives/504 proj/lfw_pure_data_train.npz')\n",
        "#trainX = concatenate((trainX, train_data['arr_0']))\n",
        "#trainy = concatenate((trainy, train_data['arr_1']))\n",
        "#savez_compressed('/content/drive/Shared drives/504 proj/lfw_mixed_data_train_cloth_high.npz', trainX, trainy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jlzc_KnOmUfW"
      },
      "source": [
        "#testX, testy = load_dataset('/content/drive/Shared drives/504 proj/OUR_DATASET/lfw_mixed_dataset/test_with_mask_masked_cloth_high/')\n",
        "#print(testX.shape, testy.shape)\n",
        "#savez_compressed('/content/drive/Shared drives/504 proj/lfw_mixed_data_test_cloth_high.npz', testX, testy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uc3bk3D07Kgo"
      },
      "source": [
        "from numpy import load\n",
        "from numpy import expand_dims\n",
        "from numpy import asarray\n",
        "from numpy import savez_compressed\n",
        "from keras.models import load_model\n",
        " \n",
        "# get the face embedding for one face\n",
        "def get_embedding(model, face_pixels):\n",
        "\tface_pixels = face_pixels.astype('float32')\n",
        "\tmean, std = face_pixels.mean(), face_pixels.std()\n",
        "\tface_pixels = (face_pixels - mean) / std\n",
        "\tsamples = expand_dims(face_pixels, axis=0)\n",
        "\tyhat = model.predict(samples)\n",
        "\treturn yhat[0]\n",
        " \n",
        "# load the face dataset\n",
        "#data = load('/content/drive/Shared drives/504 proj/pure_dataset_data.npz')\n",
        "train_data = load('/content/drive/Shared drives/504 proj/lfw_mixed_data_train_cloth_high.npz')\n",
        "test_data = load('/content/drive/Shared drives/504 proj/lfw_mixed_data_test_cloth_high.npz')\n",
        "trainX, trainy = train_data['arr_0'], train_data['arr_1']\n",
        "testX, testy = test_data['arr_0'], test_data['arr_1']\n",
        "#trainX, trainy, testX, testy = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']\n",
        "print('Loaded: ', trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
        "\n",
        "# load the facenet model\n",
        "model = load_model('/content/drive/Shared drives/504 proj/facenet_keras.h5')\n",
        "print('Loaded Model')\n",
        "\n",
        "# convert each face in the train set to an embedding\n",
        "newTrainX = list()\n",
        "for face_pixels in trainX:\n",
        "\tembedding = get_embedding(model, face_pixels)\n",
        "\tnewTrainX.append(embedding)\n",
        "newTrainX = asarray(newTrainX)\n",
        "print(newTrainX.shape)\n",
        "\n",
        "# convert each face in the test set to an embedding\n",
        "newTestX = list()\n",
        "for face_pixels in testX:\n",
        "\tembedding = get_embedding(model, face_pixels)\n",
        "\tnewTestX.append(embedding)\n",
        "newTestX = asarray(newTestX)\n",
        "print(newTestX.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cCVItR6jgC3"
      },
      "source": [
        "# model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1HnvEJU8Wlu"
      },
      "source": [
        "#savez_compressed('/content/drive/Shared drives/504 proj/faces-embeddings_lfw_mixed_dataset_cloth_high.npz', newTrainX, trainy, newTestX, testy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvAo4LL_8VFS"
      },
      "source": [
        "from numpy import load\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.svm import SVC\n",
        "# load dataset\n",
        "data = load('/content/drive/Shared drives/504 proj/faces-embeddings_lfw_mixed_dataset_cloth_high.npz')\n",
        "trainX, trainy, testX, testy = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']\n",
        "#import numpy\n",
        "#with numpy.printoptions(threshold=numpy.inf):\n",
        "#    print(trainy)\n",
        "\n",
        "# normalize input vectors\n",
        "in_encoder = Normalizer(norm='l2')\n",
        "trainX = in_encoder.transform(trainX)\n",
        "testX = in_encoder.transform(testX)\n",
        "# label encode targets\n",
        "out_encoder = LabelEncoder()\n",
        "out_encoder.fit(trainy)\n",
        "trainy = out_encoder.transform(trainy)\n",
        "testy = out_encoder.transform(testy)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02Z6v24o410G"
      },
      "source": [
        "# SVM linear\n",
        "model = SVC(kernel='linear', probability=True)\n",
        "model.fit(trainX, trainy)\n",
        "# predict\n",
        "yhat_train = model.predict(trainX)\n",
        "yhat_test = model.predict(testX)\n",
        "# score\n",
        "score_train = accuracy_score(trainy, yhat_train)\n",
        "score_test = accuracy_score(testy, yhat_test)\n",
        "print('Accuracy: train=%.3f, test=%.3f' % (score_train*100, score_test*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJOKXdyp3wOe"
      },
      "source": [
        "# SVM kernel\n",
        "model = SVC(kernel='poly', probability=True, degree=2)\n",
        "model.fit(trainX, trainy)\n",
        "yhat_train = model.predict(trainX)\n",
        "yhat_test = model.predict(testX)\n",
        "score_train = accuracy_score(trainy, yhat_train)\n",
        "score_test = accuracy_score(testy, yhat_test)\n",
        "print('Accuracy: train=%.3f, test=%.3f' % (score_train*100, score_test*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pNT80DXdSQe"
      },
      "source": [
        "# Neural network\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "NN = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=256, random_state=2).fit(trainX, trainy)\n",
        "yhat_train = NN.predict(trainX)\n",
        "yhat_test = NN.predict(testX)\n",
        "score_train = accuracy_score(trainy, yhat_train)\n",
        "score_test = accuracy_score(testy, yhat_test)\n",
        "print('Accuracy: train=%.3f, test=%.3f' % (score_train*100, score_test*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a50wIWlv5BEQ"
      },
      "source": [
        "# KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "neigh = KNeighborsClassifier(n_neighbors=4,weights='distance').fit(trainX, trainy)\n",
        "yhat_train = neigh.predict(trainX)\n",
        "yhat_test = neigh.predict(testX)\n",
        "score_train = accuracy_score(trainy, yhat_train)\n",
        "score_test = accuracy_score(testy, yhat_test)\n",
        "print('Accuracy: train=%.3f, test=%.3f' % (score_train*100, score_test*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZMCo18M9PUH"
      },
      "source": [
        "# Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier(n_estimators=256,criterion='entropy',random_state=3).fit(trainX, trainy)\n",
        "yhat_train = rf.predict(trainX)\n",
        "yhat_test = rf.predict(testX)\n",
        "score_train = accuracy_score(trainy, yhat_train)\n",
        "score_test = accuracy_score(testy, yhat_test)\n",
        "print('Accuracy: train=%.3f, test=%.3f' % (score_train*100, score_test*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuYl_Mfe2pmh"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIpVPsHy8l8-"
      },
      "source": [
        "\n",
        "from random import choice\n",
        "from numpy import load\n",
        "from numpy import expand_dims\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.svm import SVC\n",
        "from matplotlib import pyplot\n",
        "# load faces\n",
        "#data = load('/content/drive/Shared drives/504 proj/mixed_dataset_data.npz')\n",
        "train_data = load('/content/drive/Shared drives/504 proj/lfw_mixed_data_train.npz')\n",
        "test_data = load('/content/drive/Shared drives/504 proj/lfw_mixed_data_test.npz')\n",
        "testX_faces = test_data['arr_0']\n",
        "# load face embeddings\n",
        "data = load('/content/drive/Shared drives/504 proj/faces-embeddings_lfw_mixed_dataset_cloth_high.npz')\n",
        "trainX, trainy, testX, testy = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']\n",
        "# normalize input vectors\n",
        "in_encoder = Normalizer(norm='l2')\n",
        "trainX = in_encoder.transform(trainX)\n",
        "testX = in_encoder.transform(testX)\n",
        "# label encode targets\n",
        "out_encoder = LabelEncoder()\n",
        "out_encoder.fit(trainy)\n",
        "trainy = out_encoder.transform(trainy)\n",
        "testy = out_encoder.transform(testy)\n",
        "# fit model\n",
        "model = SVC(kernel='linear', probability=True)\n",
        "model.fit(trainX, trainy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfihIaVi2uPv"
      },
      "source": [
        "# test model on a random example from the test dataset\n",
        "selection = choice([i for i in range(testX.shape[0])])\n",
        "random_face_pixels = testX_faces[selection]\n",
        "random_face_emb = testX[selection]\n",
        "random_face_class = testy[selection]\n",
        "random_face_name = out_encoder.inverse_transform([random_face_class])\n",
        "# prediction for the face\n",
        "samples = expand_dims(random_face_emb, axis=0)\n",
        "yhat_class = model.predict(samples)\n",
        "\n",
        "# get name\n",
        "predict_names = out_encoder.inverse_transform(yhat_class)\n",
        "print('Predicted: %s' % (predict_names[0]))\n",
        "print('Expected: %s' % random_face_name[0])\n",
        "# plot\n",
        "pyplot.imshow(random_face_pixels)\n",
        "title = 'Predicted: %s\\n' % (predict_names[0]) + 'Expected: %s' % (random_face_name[0])\n",
        "pyplot.title(title)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mc2PoQtV3ke"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Xx5IPTMgr3e"
      },
      "source": [
        "## VGG-face\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mi8P5is2M3p8"
      },
      "source": [
        "# VGG-Face\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import ZeroPadding2D\n",
        "from tensorflow.keras.layers import Convolution2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Activation\n",
        "from keras.models import model_from_json\n",
        "from keras.models import Model\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(ZeroPadding2D((1,1),input_shape=(224, 224, 3)))\n",
        "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        " \n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        " \n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        " \n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        " \n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        " \n",
        "model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Convolution2D(2622, (1, 1)))\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.load_weights('/content/drive/Shared drives/504 proj/vgg_face_weights.h5')\n",
        "#model.summary()\n",
        "\n",
        "vgg_face_descriptor = Model(inputs=model.layers[0].input,outputs=model.layers[-2].output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVpEmgsmEPzI"
      },
      "source": [
        "#print('Loaded Data')\n",
        "#trainX, trainy = load_dataset('/content/drive/Shared drives/504 proj/data/train/', (224,224))\n",
        "#print(trainX.shape, trainy.shape)\n",
        "#testX, testy = load_dataset('/content/drive/Shared drives/504 proj/data/val/', (224,224))\n",
        "#print(testX.shape, testy.shape)\n",
        "\n",
        "#savez_compressed('/content/drive/Shared drives/504 proj/data-VGGface.npz', trainX, trainy, testX, testy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHakUYZ2Q_hf"
      },
      "source": [
        "data = load('/content/drive/Shared drives/504 proj/data-VGGface.npz')\n",
        "trainX, trainy, testX, testy = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']\n",
        "print('Loaded: ', trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
        "\n",
        "# get the face embedding for one face\n",
        "def get_embedding_VGG(model, face_pixels):\n",
        "\tface_pixels = face_pixels.astype('float32')\n",
        "\tmean, std = face_pixels.mean(), face_pixels.std()\n",
        "\tface_pixels = (face_pixels - mean) / std\n",
        "\tsamples = expand_dims(face_pixels, axis=0)\n",
        "\tyhat = vgg_face_descriptor.predict(samples)\n",
        "\treturn yhat[0]\n",
        "\n",
        "newTrainX = list()\n",
        "for face_pixels in trainX:\n",
        "\tembedding = get_embedding_VGG(model, face_pixels)\n",
        "\tnewTrainX.append(embedding)\n",
        "newTrainX = asarray(newTrainX)\n",
        "print(newTrainX.shape)\n",
        "\n",
        "# convert each face in the test set to an embedding\n",
        "newTestX = list()\n",
        "for face_pixels in testX:\n",
        "\tembedding = get_embedding_VGG(model, face_pixels)\n",
        "\tnewTestX.append(embedding)\n",
        "newTestX = asarray(newTestX)\n",
        "print(newTestX.shape)\n",
        "\n",
        "savez_compressed('/content/drive/Shared drives/504 proj/faces-embeddings-VGGface.npz', newTrainX, trainy, newTestX, testy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyIzScb2XJsa"
      },
      "source": [
        "from numpy import load\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.svm import SVC\n",
        "# load dataset\n",
        "data = load('/content/drive/Shared drives/504 proj/faces-embeddings-VGGface.npz')\n",
        "trainX, trainy, testX, testy = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']\n",
        "# normalize input vectors\n",
        "in_encoder = Normalizer(norm='l2')\n",
        "trainX = in_encoder.transform(trainX)\n",
        "testX = in_encoder.transform(testX)\n",
        "# label encode targets\n",
        "out_encoder = LabelEncoder()\n",
        "out_encoder.fit(trainy)\n",
        "trainy = out_encoder.transform(trainy)\n",
        "testy = out_encoder.transform(testy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3ma_0SrXP4x"
      },
      "source": [
        "# SVM linear\n",
        "model = SVC(kernel='linear', probability=True)\n",
        "model.fit(trainX, trainy)\n",
        "# predict\n",
        "yhat_train = model.predict(trainX)\n",
        "yhat_test = model.predict(testX)\n",
        "# score\n",
        "score_train = accuracy_score(trainy, yhat_train)\n",
        "score_test = accuracy_score(testy, yhat_test)\n",
        "print('Accuracy: train=%.3f, test=%.3f' % (score_train*100, score_test*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DU-VZD6tXYXf"
      },
      "source": [
        "# SVM kernel\n",
        "model = SVC(kernel='poly', probability=True, degree=3)\n",
        "model.fit(trainX, trainy)\n",
        "yhat_train = model.predict(trainX)\n",
        "yhat_test = model.predict(testX)\n",
        "score_train = accuracy_score(trainy, yhat_train)\n",
        "score_test = accuracy_score(testy, yhat_test)\n",
        "print('Accuracy: train=%.3f, test=%.3f' % (score_train*100, score_test*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtPR6BaYXf9U"
      },
      "source": [
        "# Neural network\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "NN = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=2262, random_state=4).fit(trainX, trainy)\n",
        "yhat_train = NN.predict(trainX)\n",
        "yhat_test = NN.predict(testX)\n",
        "score_train = accuracy_score(trainy, yhat_train)\n",
        "score_test = accuracy_score(testy, yhat_test)\n",
        "print('Accuracy: train=%.3f, test=%.3f' % (score_train*100, score_test*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn5wLwBjbjJ7"
      },
      "source": [
        "# KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "neigh = KNeighborsClassifier(n_neighbors=3,weights='distance').fit(trainX, trainy)\n",
        "yhat_train = NN.predict(trainX)\n",
        "yhat_test = NN.predict(testX)\n",
        "score_train = accuracy_score(trainy, yhat_train)\n",
        "score_test = accuracy_score(testy, yhat_test)\n",
        "print('Accuracy: train=%.3f, test=%.3f' % (score_train*100, score_test*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a54WJje3bodg"
      },
      "source": [
        "# Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier(n_estimators=128,criterion='entropy',random_state=2).fit(trainX, trainy)\n",
        "yhat_train = rf.predict(trainX)\n",
        "yhat_test = rf.predict(testX)\n",
        "score_train = accuracy_score(trainy, yhat_train)\n",
        "score_test = accuracy_score(testy, yhat_test)\n",
        "print('Accuracy: train=%.3f, test=%.3f' % (score_train*100, score_test*100))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}